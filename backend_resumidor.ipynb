{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVLlDV2vYMy_"
      },
      "source": [
        "# 1 - Instalación de Dependencias\n",
        "Instalamos FastAPI, Ngrok, Whisper y las herramientas de sistema necesarias (FFmpeg)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3-xthhglXkOW",
        "outputId": "aeb8fc86-8891-4afd-9c4d-e9729ff6e92f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (0.0.21)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "100 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 100 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# 1. Instalar dependencias de Python\n",
        "!pip install fastapi uvicorn pyngrok python-multipart openai-whisper requests\n",
        "\n",
        "# 2. Instalar herramientas del sistema (FFmpeg y ZSTD para descomprimir)\n",
        "# AGREGADO: 'zstd' para solucionar tu error\n",
        "!sudo apt update && sudo apt install ffmpeg zstd -y\n",
        "\n",
        "# 3. Instalar Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T07mFN7eBVI"
      },
      "source": [
        "# verificar gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l1_weGh-eEBW",
        "outputId": "4e32cc76-9215-4c8d-a9e4-9ced131d34d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 100 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 1s (316 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 121710 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Wed Jan 21 12:12:48 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# 1. Instalar la herramienta para detectar hardware (lspci)\n",
        "!sudo apt install pciutils -y\n",
        "\n",
        "# 2. Re-ejecutar la instalación de Ollama (ahora sí detectará la GPU)\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 3. VERIFICACIÓN FINAL:\n",
        "# Si esto muestra una tabla que dice \"Tesla T4\", ¡estás listo!\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Grwth6IYR7X"
      },
      "source": [
        "# 2 - Iniciar Ollama y Descargar el Modelo\n",
        "Aquí hay truco: Ollama funciona como un servidor, así que debemos lanzarlo en segundo plano (subprocess) antes de pedirle que descargue el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RtnbQslYe_t",
        "outputId": "29c381a4-e8de-4d96-edf4-e3f35ba890f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Iniciando servidor Ollama...\n",
            "⬇ Descargando DeepSeek R1 (esto puede tardar unos minutos)...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            ":) Modelo listo.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# 1. Iniciar el servidor de Ollama en segundo plano\n",
        "print(\" Iniciando servidor Ollama...\")\n",
        "process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(5)  # Esperar a que arranque\n",
        "\n",
        "# 2. Descargar el modelo DeepSeek R1 (versión 8b optimizada)\n",
        "print(\"⬇ Descargando DeepSeek R1 (esto puede tardar unos minutos)...\")\n",
        "!ollama pull deepseek-r1:8b\n",
        "print(\":) Modelo listo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkKpQ-hqfBJT"
      },
      "source": [
        "# 3 - codigo 'main.py' dentro del colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keB4I56rfF3X",
        "outputId": "4182c11c-8f4b-4ad4-b947-72ad641e3048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import whisper\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Permitir CORS para que tu GitHub Page pueda hablar con Colab\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Cargar Whisper usando la GPU (cuda)\n",
        "print(\"Cargando Whisper en GPU...\")\n",
        "model = whisper.load_model(\"base\", device=\"cuda\")\n",
        "print(\"Whisper cargado.\")\n",
        "\n",
        "@app.post(\"/procesar-reunion\")\n",
        "async def procesar_reunion(file: UploadFile = File(...)):\n",
        "    temp_filename = f\"temp_{file.filename}\"\n",
        "\n",
        "    try:\n",
        "        # 1. Guardar audio\n",
        "        with open(temp_filename, \"wb\") as buffer:\n",
        "            buffer.write(await file.read())\n",
        "\n",
        "        # 2. Transcribir\n",
        "        result = model.transcribe(temp_filename)\n",
        "        transcription_text = result[\"text\"]\n",
        "\n",
        "        # 3. Resumir con DeepSeek (vía Ollama local en Colab)\n",
        "        # DeepSeek R1 es un modelo de razonamiento, a veces piensa mucho (\"<think>\").\n",
        "        # Le pedimos que solo nos de el resumen final.\n",
        "        prompt = f\"Analiza la siguiente transcripción de reunión. Genera un resumen ejecutivo en español con viñetas claras. Ignora tus pensamientos internos en la salida final.\\n\\nTexto: {transcription_text}\"\n",
        "\n",
        "        response = requests.post('http://localhost:11434/api/generate', json={\n",
        "            \"model\": \"deepseek-r1:8b\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        })\n",
        "\n",
        "        summary = response.json()['response']\n",
        "\n",
        "        # Limpiar etiquetas de pensamiento de DeepSeek R1 si aparecen\n",
        "        if \"</think>\" in summary:\n",
        "            summary = summary.split(\"</think>\")[-1].strip()\n",
        "\n",
        "        return {\n",
        "            \"transcription\": transcription_text,\n",
        "            \"summary\": summary\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(temp_filename):\n",
        "            os.remove(temp_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUfrHuwSv8WM",
        "outputId": "f152f095-7a61-49d1-bbb2-fba28f7e33b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import whisper\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "print(\" Cargando Whisper (GPU)...\")\n",
        "model = whisper.load_model(\"base\", device=\"cuda\")\n",
        "print(\" Whisper cargado y listo.\")\n",
        "\n",
        "# Crear carpeta para guardar audios visibles\n",
        "os.makedirs(\"grabaciones\", exist_ok=True)\n",
        "\n",
        "@app.post(\"/procesar-reunion\")\n",
        "async def procesar_reunion(file: UploadFile = File(...)):\n",
        "    # Generar nombre único con hora\n",
        "    filename = f\"grabaciones/debug_{int(time.time())}.wav\"\n",
        "\n",
        "    print(f\"\\n 1. RECIBIENDO ARCHIVO...\")\n",
        "\n",
        "    try:\n",
        "        # Guardar audio\n",
        "        with open(filename, \"wb\") as buffer:\n",
        "            buffer.write(await file.read())\n",
        "        print(f\" 2. AUDIO GUARDADO EN: {filename}\")\n",
        "        print(\"   (Busca este archivo en la carpeta de la izquierda en Colab)\")\n",
        "\n",
        "        # Transcribir\n",
        "        print(\" 3. INICIANDO TRANSCRIPCIÓN CON WHISPER...\")\n",
        "        result = model.transcribe(filename)\n",
        "        transcription_text = result[\"text\"]\n",
        "\n",
        "        # --- DEBUG VISUAL ---\n",
        "        print(f\" TEXTO DETECTADO: [{transcription_text}]\")\n",
        "\n",
        "        if not transcription_text.strip():\n",
        "            print(\" ADVERTENCIA: Whisper no escuchó nada (texto vacío).\")\n",
        "            return {\n",
        "                \"transcription\": \"(Silencio o Ruido no detectado)\",\n",
        "                \"summary\": \"No se detectó voz humana en el audio.\"\n",
        "            }\n",
        "\n",
        "        # Resumir con DeepSeek\n",
        "        print(\" 4. ENVIANDO A DEEPSEEK R1...\")\n",
        "        prompt = f\"Resume esto brevemente: {transcription_text}\"\n",
        "\n",
        "        response = requests.post('http://localhost:11434/api/generate', json={\n",
        "            \"model\": \"deepseek-r1:8b\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        })\n",
        "\n",
        "        summary = response.json()['response']\n",
        "        # Limpieza de tags <think>\n",
        "        if \"</think>\" in summary:\n",
        "            summary = summary.split(\"</think>\")[-1].strip()\n",
        "\n",
        "        print(f\" RESUMEN GENERADO: {summary[:50]}...\") # Solo mostramos el inicio\n",
        "\n",
        "        return {\n",
        "            \"transcription\": transcription_text,\n",
        "            \"summary\": summary\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR CRÍTICO EN BACKEND: {str(e)}\")\n",
        "        return {\"transcription\": f\"Error: {str(e)}\", \"summary\": \"Error en el servidor\"}\n",
        "\n",
        "    # ==========================================\n",
        "    # ZONA DE INTERRUPTOR DE GUARDADO\n",
        "    # ==========================================\n",
        "    finally:\n",
        "        # MODO ACTUAL: BORRAR AUDIOS (Privacidad / Ahorro de espacio)\n",
        "        # Si quieres que se GUARDEN para pruebas, pon un \"#\" al inicio de las 2 líneas de abajo:\n",
        "\n",
        "        if os.path.exists(filename):\n",
        "             os.remove(filename)\n",
        "             print(f\" Limpieza automática: {filename} eliminado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "s2fkXQEjhqBC"
      },
      "outputs": [],
      "source": [
        "!pkill ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISms8Uc6fPDd"
      },
      "source": [
        "# 4 - Exponer a NGROK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUIOPnp_xCXw",
        "outputId": "cfe43316-2f4b-480e-fa4a-51159dbafb5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " DIAGNÓSTICO: Revisando estado de DeepSeek/Ollama...\n",
            " Ollama está APAGADO. Iniciando reactivación...\n",
            " Esperando a que Ollama despierte (10 segundos)...\n",
            " ¡Ollama revivió exitosamente!\n",
            " Verificando que el modelo 'deepseek-r1:8b' esté listo...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\n",
            " TODO LISTO. Ahora vuelve a ejecutar la celda del Servidor (la última).\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "print(\" DIAGNÓSTICO: Revisando estado de DeepSeek/Ollama...\")\n",
        "\n",
        "# 1. Intentar conectar para ver si está vivo\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434\")\n",
        "    print(\" Ollama ya está corriendo.\")\n",
        "except:\n",
        "    print(\" Ollama está APAGADO. Iniciando reactivación...\")\n",
        "\n",
        "    # 2. Iniciar el servidor en segundo plano\n",
        "    # Usamos nohup para que sobreviva mejor en Colab\n",
        "    process = subprocess.Popen(\"nohup ollama serve > ollama.log 2>&1 &\", shell=True)\n",
        "\n",
        "    print(\" Esperando a que Ollama despierte (10 segundos)...\")\n",
        "    time.sleep(10)\n",
        "\n",
        "    # 3. Verificar de nuevo\n",
        "    try:\n",
        "        requests.get(\"http://localhost:11434\")\n",
        "        print(\" ¡Ollama revivió exitosamente!\")\n",
        "    except:\n",
        "        print(\" ALERTA: Ollama está tardando en iniciar. Esperando 10 segundos más...\")\n",
        "        time.sleep(10)\n",
        "\n",
        "# 4. Asegurar que el modelo DeepSeek está cargado en memoria\n",
        "print(\" Verificando que el modelo 'deepseek-r1:8b' esté listo...\")\n",
        "# Ejecutamos un 'pull' rápido. Si ya existe, no descargará nada, solo verificará.\n",
        "!ollama pull deepseek-r1:8b\n",
        "\n",
        "print(\"\\n TODO LISTO. Ahora vuelve a ejecutar la celda del Servidor (la última).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13lZKyqujLKl",
        "outputId": "22d0cd12-b477-4b76-eeca-cdd5394004a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Matando procesos viejos...\n",
            "\n",
            "==================================================================\n",
            " TU URL DE BACKEND ES:  https://joi-excludable-ulises.ngrok-free.dev\n",
            "==================================================================\n",
            "\n",
            "  Copia esa URL (HTTPS) en tu index.html AHORA.\n",
            "... Iniciando servidor Uvicorn... (Espera a que diga 'Application startup complete')\n",
            " Cargando Whisper (GPU)...\n",
            " Whisper cargado y listo.\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m30840\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\n",
            " 1. RECIBIENDO ARCHIVO...\n",
            " 2. AUDIO GUARDADO EN: grabaciones/debug_1769002578.wav\n",
            "   (Busca este archivo en la carpeta de la izquierda en Colab)\n",
            " 3. INICIANDO TRANSCRIPCIÓN CON WHISPER...\n",
            " TEXTO DETECTADO: [ Ahora sí debería funcionar donde voy a ser que este un bocano al teclado pero no encuentro]\n",
            " 4. ENVIANDO A DEEPSEEK R1...\n",
            " RESUMEN GENERADO: Ahora sí debería funcionar donde voy a ser que **h...\n",
            "\u001b[32mINFO\u001b[0m:     200.29.139.30:0 - \"\u001b[1mPOST /procesar-reunion HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\n",
            " 1. RECIBIENDO ARCHIVO...\n",
            " 2. AUDIO GUARDADO EN: grabaciones/debug_1769002739.wav\n",
            "   (Busca este archivo en la carpeta de la izquierda en Colab)\n",
            " 3. INICIANDO TRANSCRIPCIÓN CON WHISPER...\n",
            " TEXTO DETECTADO: [ Ahora voy a... Como lo del Nollens Ya Pero... Lo acabas de hacer con el Google Colab Ahora voy a... Ahora voy a contigo y te voy a explicarme como fue así como... No tal que está en la nota de la tal No entendiste nada de la... No, no, no, no, sí, entendiste como... Me voy a empezar a explicar porque... Pero... Es como... Básicamente sale en toda la cartera de cliente De Clariveterre Y se quiso hacer como una tabla donde mostrar a la dissen... La cantidad de insidencias que tenían ese cliente por mes Y no sé para según marcan y según tecnologías Sin Nadie en la misma fila te indicabas Es que tenía solo claro, solo vete de google.com y en tema de tecnologías es que tenía una tecnología a dos o tres y después al final tenía la suma de la incidencia de la naturaleza.]\n",
            " 4. ENVIANDO A DEEPSEEK R1...\n",
            " RESUMEN GENERADO: Claro, aquí tienes un resumen breve de lo que pare...\n",
            "\u001b[32mINFO\u001b[0m:     200.29.139.30:0 - \"\u001b[1mPOST /procesar-reunion HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m30840\u001b[0m]\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# --- PASO 1: LIMPIEZA TOTAL ---\n",
        "print(\" Matando procesos viejos...\")\n",
        "!pkill ngrok\n",
        "!pkill uvicorn\n",
        "\n",
        "# --- PASO 2: CONFIGURAR NGROK ---\n",
        "NGROK_TOKEN = \"38Wff95yfqrA0VesjwgaYRyctLnB_49Axs\"  # <--- ¡NO OLVIDAR ESTO!\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# --- PASO 3: ABRIR EL TÚNEL ---\n",
        "# Abrimos el túnel en segundo plano antes de iniciar el servidor\n",
        "try:\n",
        "    # Si usas dominio estático, descomenta y usa esta línea:\n",
        "    # public_url = ngrok.connect(8000, domain=\"tu-dominio.ngrok-free.app\").public_url\n",
        "\n",
        "    # Si usas dominio aleatorio:\n",
        "    # public_url = ngrok.connect(8000).public_url\n",
        "    # Si usas dominio estatico:\n",
        "    public_url = ngrok.connect(8000, domain=\"br-ulises.ngrok-free.dev\").public_url\n",
        "\n",
        "\n",
        "    print(f\"\\n==================================================================\")\n",
        "    print(f\" TU URL DE BACKEND ES:  {public_url}\")\n",
        "    print(f\"==================================================================\\n\")\n",
        "    print(\"  Copia esa URL (HTTPS) en tu index.html AHORA.\")\n",
        "    print(\"... Iniciando servidor Uvicorn... (Espera a que diga 'Application startup complete')\")\n",
        "\n",
        "    # Damos un segundo para que leas la URL\n",
        "    time.sleep(2)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error con Ngrok: {e}\")\n",
        "\n",
        "# --- PASO 4: INICIAR SERVIDOR (MODO COMANDO) ---\n",
        "# Usamos '!' para correrlo como si fuera la terminal de Linux.\n",
        "# Esto evita el error \"asyncio.run() cannot be called...\"\n",
        "!uvicorn main:app --host 0.0.0.0 --port 8000"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
